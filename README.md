# Reddit Data Pipeline using Apache Airflow (Astro)

This project demonstrates an **end-to-end data engineering pipeline**
built using **Apache Airflow (Astronomer Runtime)** to extract Reddit
data, transform it, load it into PostgreSQL, and model it using
dimensional modeling principles.\
The project is designed to be **production-style**, modular, and secure.

------------------------------------------------------------------------

## üöÄ Architecture Overview

**Flow:**\
Reddit API ‚Üí Raw JSON ‚Üí Refined CSV ‚Üí Staging Table ‚Üí Incremental Load ‚Üí
Fact & Dimension Tables ‚Üí Cleanup

![Architecture Diagram](pictures/architecture_diagram.jpg)

**Core Components:** - Apache Airflow (Astro CLI) - Reddit API (PRAW) -
PostgreSQL (Airflow-provided) - Pandas / PySpark-style transformations -
SQL-based dimensional modeling

------------------------------------------------------------------------

## üß∞ Prerequisites

### 1Ô∏è‚É£ Install Docker

Ensure Docker Desktop is installed and running.

üëâ https://www.docker.com/products/docker-desktop/

------------------------------------------------------------------------

### 2Ô∏è‚É£ Install Astro CLI

#### Windows (PowerShell)

``` powershell
winget install -e --id Astronomer.Astro
```

Verify:

``` bash
astro version
```

------------------------------------------------------------------------

## üìÅ Project Setup

### 3Ô∏è‚É£ Clone the Repository

``` bash
git clone <your-github-repo-url>
cd <project-folder>
```

------------------------------------------------------------------------

### 4Ô∏è‚É£ Start Airflow (Astro)

Run this **from the project root directory** (where `astro-project.yaml`
exists):

``` bash
astro dev start
```

This will: - Start Airflow Webserver - Start Scheduler - Start
PostgreSQL (Docker) - Mount dags/, include/, data/ folders

Airflow UI: üëâ http://localhost:8080\
Username: `admin`\
Password: `admin`

------------------------------------------------------------------------

## üîê Secure Configuration (VERY IMPORTANT)

### 5Ô∏è‚É£ Create Reddit API Connection

**Airflow UI ‚Üí Admin ‚Üí Connections ‚Üí Add Connection**

-   **Conn ID:** `reddit_api`
-   **Conn Type:** `Generic`
-   **Login:** `<client_id>`
-   **Password:** `<client_secret>`
-   **Extra (JSON):**

``` json
{
  "user_agent": "python:DataAnalyzer:1.0 (by /u/your_reddit_username)"
}
```

‚úÖ This keeps secrets out of code

------------------------------------------------------------------------

### 6Ô∏è‚É£ Create Dynamic Reddit Config Variable

**Airflow UI ‚Üí Admin ‚Üí Variables ‚Üí Add Variable**

-   **Key:** `reddit_config`
-   **Value (JSON):**

``` json
{
  "subreddits": ["relationshipindia", "india", "mentalhealth"],
  "post_type": "new",
  "limit": 1000
}
```

------------------------------------------------------------------------

## üõë PostgreSQL Port Conflict Fix (Windows)

Before starting Astro, **STOP any local PostgreSQL service** on port
`5432`.

### Option 1: Services

1.  Win + R ‚Üí `services.msc`
2.  Stop:
    -   `postgresql-x64-*`

### Option 2: Command Line (Admin)

``` powershell
net stop postgresql-x64-15
```

------------------------------------------------------------------------

## üêò PostgreSQL Setup (Airflow Provided)

### 7Ô∏è‚É£ Create PostgreSQL Connection

**Airflow UI ‚Üí Admin ‚Üí Connections ‚Üí Add Connection**

-   **Conn ID:** `postgres_reddit`
-   **Conn Type:** `Postgres`
-   **Host:** `your_hostname`
-   **Schema:** `af_reddit`
-   **Login:** `your_id`
-   **Password:** `your_password`
-   **Port:** `5432`

------------------------------------------------------------------------

## üß© DAGs Overview

### 1Ô∏è‚É£ reddit_extract_dag

-   Extracts Reddit posts using PRAW
-   Stores raw JSON files
-   Uses secure Airflow connection

### 2Ô∏è‚É£ reddit_transform_dag

-   Reads raw JSON
-   Applies sentiment & intent logic
-   Writes refined CSV
-   Gracefully succeeds even if no files exist

### 3Ô∏è‚É£ reddit_load_dag

-   Loads refined CSV into `stg_reddit_posts`
-   Uses Airflow Postgres hook

### 4Ô∏è‚É£ reddit_incremental_load_dag

-   Inserts **only new posts**
-   Deduplicates using Reddit post ID

### 5Ô∏è‚É£ reddit_dimensional_model_dag

Creates: - `fact_reddit_posts` - `dim_subreddit` - `dim_author` -
`dim_raw_source_file` - `dim_refined_source_file`

### 6Ô∏è‚É£ reddit_cleanup_dag

-   Truncates staging table
-   Deletes raw & refined files (adhoc)

------------------------------------------------------------------------

## üß± Core Tables

### 1Ô∏è‚É£ stg_reddit_posts

- Staging table that stores raw Reddit post data ingested from the source files with minimal transformations, used for data validation and cleansing before further processing.

### 2Ô∏è‚É£ reddit_posts (Unique Records)

- Deduplicated intermediate table that retains only unique Reddit posts based on business keys, ensuring no duplicate records flow into downstream fact tables.

### 3Ô∏è‚É£ fact_reddit_posts

- Fact table containing the core measurable Reddit post data (such as scores, comments count, sentiment, and timestamps), linked to dimension tables for analytical reporting.

### 4Ô∏è‚É£ dim_author

- Dimension table that stores unique Reddit author details, providing descriptive context for analyzing posts by user.

### 5Ô∏è‚É£ dim_subreddit

- Dimension table that captures subreddit metadata, enabling analysis of Reddit posts by community or topic.

### 6Ô∏è‚É£ dim_raw_source_file

- Dimension table that tracks metadata of raw ingested source files, supporting data lineage, auditing, and ingestion traceability.

### 7Ô∏è‚É£ dim_refined_source_file

- Dimension table that stores metadata for refined/processed source files, enabling end-to-end data lineage tracking from raw ingestion to curated datasets.

------------------------------------------------------------------------

## üß† Warehouse Design Decisions

-   **Reddit ID** used as natural key
-   URLs kept in fact table (high cardinality)
-   Dimensions kept slim (best practice)
-   Incremental loads for scalability
-   Cleanup isolated into its own DAG

------------------------------------------------------------------------

## ‚ñ∂Ô∏è How to Run the Project

1.  Start Docker
2.  Run:

``` bash
astro dev start
```

3.  Open Airflow UI
4.  Configure connections & variables
5.  Trigger DAGs in order:
    -   extract ‚Üí transform ‚Üí load ‚Üí incremental ‚Üí dimensional ‚Üí cleanup

------------------------------------------------------------------------

## üìä Future Enhancements

-   Power BI dashboards
-   dbt transformations
-   Data quality checks
-   CI/CD for DAGs
-   Schema evolution handling

------------------------------------------------------------------------

## üë§ Author

**Yash Tare**\
Data Engineer\
Tech Stack: Airflow \| Python \| SQL \| PostgreSQL \| Azure \|
Databricks

------------------------------------------------------------------------

‚≠ê If you like this project, give it a star on GitHub!
